<Figure size 640x480 with 64 Axes>
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 16, 16]           9,408
       BatchNorm2d-2           [-1, 64, 16, 16]             128
              ReLU-3           [-1, 64, 16, 16]               0
         MaxPool2d-4             [-1, 64, 8, 8]               0
            Conv2d-5             [-1, 64, 8, 8]           4,096
       BatchNorm2d-6             [-1, 64, 8, 8]             128
              ReLU-7             [-1, 64, 8, 8]               0
            Linear-8               [-1, 64, 16]           1,024
           Softmax-9               [-1, 64, 16]               0
           Linear-10               [-1, 64, 16]           1,024
      BatchNorm1d-11               [-1, 64, 16]             128
           Linear-12               [-1, 64, 64]           4,096
      BatchNorm2d-13            [-1, 16, 64, 4]              32
      LambdaLayer-14             [-1, 64, 8, 8]               0
      BatchNorm2d-15             [-1, 64, 8, 8]             128
             ReLU-16             [-1, 64, 8, 8]               0
           Conv2d-17            [-1, 256, 8, 8]          16,384
      BatchNorm2d-18            [-1, 256, 8, 8]             512
           Conv2d-19            [-1, 256, 8, 8]          16,384
      BatchNorm2d-20            [-1, 256, 8, 8]             512
             ReLU-21            [-1, 256, 8, 8]               0
       Bottleneck-22            [-1, 256, 8, 8]               0
           Conv2d-23             [-1, 64, 8, 8]          16,384
      BatchNorm2d-24             [-1, 64, 8, 8]             128
             ReLU-25             [-1, 64, 8, 8]               0
           Linear-26               [-1, 64, 16]           1,024
          Softmax-27               [-1, 64, 16]               0
           Linear-28               [-1, 64, 16]           1,024
      BatchNorm1d-29               [-1, 64, 16]             128
           Linear-30               [-1, 64, 64]           4,096
      BatchNorm2d-31            [-1, 16, 64, 4]              32
      LambdaLayer-32             [-1, 64, 8, 8]               0
      BatchNorm2d-33             [-1, 64, 8, 8]             128
             ReLU-34             [-1, 64, 8, 8]               0
           Conv2d-35            [-1, 256, 8, 8]          16,384
      BatchNorm2d-36            [-1, 256, 8, 8]             512
             ReLU-37            [-1, 256, 8, 8]               0
       Bottleneck-38            [-1, 256, 8, 8]               0
           Conv2d-39             [-1, 64, 8, 8]          16,384
      BatchNorm2d-40             [-1, 64, 8, 8]             128
             ReLU-41             [-1, 64, 8, 8]               0
           Linear-42               [-1, 64, 16]           1,024
          Softmax-43               [-1, 64, 16]               0
           Linear-44               [-1, 64, 16]           1,024
      BatchNorm1d-45               [-1, 64, 16]             128
           Linear-46               [-1, 64, 64]           4,096
      BatchNorm2d-47            [-1, 16, 64, 4]              32
      LambdaLayer-48             [-1, 64, 8, 8]               0
      BatchNorm2d-49             [-1, 64, 8, 8]             128
             ReLU-50             [-1, 64, 8, 8]               0
           Conv2d-51            [-1, 256, 8, 8]          16,384
      BatchNorm2d-52            [-1, 256, 8, 8]             512
             ReLU-53            [-1, 256, 8, 8]               0
       Bottleneck-54            [-1, 256, 8, 8]               0
           Conv2d-55            [-1, 128, 8, 8]          32,768
      BatchNorm2d-56            [-1, 128, 8, 8]             256
             ReLU-57            [-1, 128, 8, 8]               0
           Linear-58               [-1, 64, 16]           2,048
          Softmax-59               [-1, 64, 16]               0
           Linear-60               [-1, 64, 32]           4,096
      BatchNorm1d-61               [-1, 64, 32]             128
           Linear-62               [-1, 64, 64]           8,192
      BatchNorm2d-63            [-1, 16, 64, 4]              32
      LambdaLayer-64            [-1, 128, 8, 8]               0
        AvgPool2d-65            [-1, 128, 4, 4]               0
      BatchNorm2d-66            [-1, 128, 4, 4]             256
             ReLU-67            [-1, 128, 4, 4]               0
           Conv2d-68            [-1, 512, 4, 4]          65,536
      BatchNorm2d-69            [-1, 512, 4, 4]           1,024
           Conv2d-70            [-1, 512, 4, 4]         131,072
      BatchNorm2d-71            [-1, 512, 4, 4]           1,024
             ReLU-72            [-1, 512, 4, 4]               0
       Bottleneck-73            [-1, 512, 4, 4]               0
           Conv2d-74            [-1, 128, 4, 4]          65,536
      BatchNorm2d-75            [-1, 128, 4, 4]             256
             ReLU-76            [-1, 128, 4, 4]               0
           Linear-77               [-1, 16, 16]           2,048
          Softmax-78               [-1, 16, 16]               0
           Linear-79               [-1, 16, 32]           4,096
      BatchNorm1d-80               [-1, 16, 32]              32
           Linear-81               [-1, 16, 64]           8,192
      BatchNorm2d-82            [-1, 16, 16, 4]              32
      LambdaLayer-83            [-1, 128, 4, 4]               0
      BatchNorm2d-84            [-1, 128, 4, 4]             256
             ReLU-85            [-1, 128, 4, 4]               0
           Conv2d-86            [-1, 512, 4, 4]          65,536
      BatchNorm2d-87            [-1, 512, 4, 4]           1,024
             ReLU-88            [-1, 512, 4, 4]               0
       Bottleneck-89            [-1, 512, 4, 4]               0
           Conv2d-90            [-1, 128, 4, 4]          65,536
      BatchNorm2d-91            [-1, 128, 4, 4]             256
             ReLU-92            [-1, 128, 4, 4]               0
           Linear-93               [-1, 16, 16]           2,048
          Softmax-94               [-1, 16, 16]               0
           Linear-95               [-1, 16, 32]           4,096
      BatchNorm1d-96               [-1, 16, 32]              32
           Linear-97               [-1, 16, 64]           8,192
      BatchNorm2d-98            [-1, 16, 16, 4]              32
      LambdaLayer-99            [-1, 128, 4, 4]               0
     BatchNorm2d-100            [-1, 128, 4, 4]             256
            ReLU-101            [-1, 128, 4, 4]               0
          Conv2d-102            [-1, 512, 4, 4]          65,536
     BatchNorm2d-103            [-1, 512, 4, 4]           1,024
            ReLU-104            [-1, 512, 4, 4]               0
      Bottleneck-105            [-1, 512, 4, 4]               0
          Conv2d-106            [-1, 128, 4, 4]          65,536
     BatchNorm2d-107            [-1, 128, 4, 4]             256
            ReLU-108            [-1, 128, 4, 4]               0
          Linear-109               [-1, 16, 16]           2,048
         Softmax-110               [-1, 16, 16]               0
          Linear-111               [-1, 16, 32]           4,096
     BatchNorm1d-112               [-1, 16, 32]              32
          Linear-113               [-1, 16, 64]           8,192
     BatchNorm2d-114            [-1, 16, 16, 4]              32
     LambdaLayer-115            [-1, 128, 4, 4]               0
     BatchNorm2d-116            [-1, 128, 4, 4]             256
            ReLU-117            [-1, 128, 4, 4]               0
          Conv2d-118            [-1, 512, 4, 4]          65,536
     BatchNorm2d-119            [-1, 512, 4, 4]           1,024
            ReLU-120            [-1, 512, 4, 4]               0
      Bottleneck-121            [-1, 512, 4, 4]               0
          Conv2d-122            [-1, 256, 4, 4]         131,072
     BatchNorm2d-123            [-1, 256, 4, 4]             512
            ReLU-124            [-1, 256, 4, 4]               0
          Linear-125               [-1, 16, 16]           4,096
         Softmax-126               [-1, 16, 16]               0
          Linear-127               [-1, 16, 64]          16,384
     BatchNorm1d-128               [-1, 16, 64]              32
          Linear-129               [-1, 16, 64]          16,384
     BatchNorm2d-130            [-1, 16, 16, 4]              32
     LambdaLayer-131            [-1, 256, 4, 4]               0
       AvgPool2d-132            [-1, 256, 2, 2]               0
     BatchNorm2d-133            [-1, 256, 2, 2]             512
            ReLU-134            [-1, 256, 2, 2]               0
          Conv2d-135           [-1, 1024, 2, 2]         262,144
     BatchNorm2d-136           [-1, 1024, 2, 2]           2,048
          Conv2d-137           [-1, 1024, 2, 2]         524,288
     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048
            ReLU-139           [-1, 1024, 2, 2]               0
      Bottleneck-140           [-1, 1024, 2, 2]               0
          Conv2d-141            [-1, 256, 2, 2]         262,144
     BatchNorm2d-142            [-1, 256, 2, 2]             512
            ReLU-143            [-1, 256, 2, 2]               0
          Linear-144                [-1, 4, 16]           4,096
         Softmax-145                [-1, 4, 16]               0
          Linear-146                [-1, 4, 64]          16,384
     BatchNorm1d-147                [-1, 4, 64]               8
          Linear-148                [-1, 4, 64]          16,384
     BatchNorm2d-149             [-1, 16, 4, 4]              32
     LambdaLayer-150            [-1, 256, 2, 2]               0
     BatchNorm2d-151            [-1, 256, 2, 2]             512
            ReLU-152            [-1, 256, 2, 2]               0
          Conv2d-153           [-1, 1024, 2, 2]         262,144
     BatchNorm2d-154           [-1, 1024, 2, 2]           2,048
            ReLU-155           [-1, 1024, 2, 2]               0
      Bottleneck-156           [-1, 1024, 2, 2]               0
          Conv2d-157            [-1, 256, 2, 2]         262,144
     BatchNorm2d-158            [-1, 256, 2, 2]             512
            ReLU-159            [-1, 256, 2, 2]               0
          Linear-160                [-1, 4, 16]           4,096
         Softmax-161                [-1, 4, 16]               0
          Linear-162                [-1, 4, 64]          16,384
     BatchNorm1d-163                [-1, 4, 64]               8
          Linear-164                [-1, 4, 64]          16,384
     BatchNorm2d-165             [-1, 16, 4, 4]              32
     LambdaLayer-166            [-1, 256, 2, 2]               0
     BatchNorm2d-167            [-1, 256, 2, 2]             512
            ReLU-168            [-1, 256, 2, 2]               0
          Conv2d-169           [-1, 1024, 2, 2]         262,144
     BatchNorm2d-170           [-1, 1024, 2, 2]           2,048
            ReLU-171           [-1, 1024, 2, 2]               0
      Bottleneck-172           [-1, 1024, 2, 2]               0
          Conv2d-173            [-1, 256, 2, 2]         262,144
     BatchNorm2d-174            [-1, 256, 2, 2]             512
            ReLU-175            [-1, 256, 2, 2]               0
          Linear-176                [-1, 4, 16]           4,096
         Softmax-177                [-1, 4, 16]               0
          Linear-178                [-1, 4, 64]          16,384
     BatchNorm1d-179                [-1, 4, 64]               8
          Linear-180                [-1, 4, 64]          16,384
     BatchNorm2d-181             [-1, 16, 4, 4]              32
     LambdaLayer-182            [-1, 256, 2, 2]               0
     BatchNorm2d-183            [-1, 256, 2, 2]             512
            ReLU-184            [-1, 256, 2, 2]               0
          Conv2d-185           [-1, 1024, 2, 2]         262,144
     BatchNorm2d-186           [-1, 1024, 2, 2]           2,048
            ReLU-187           [-1, 1024, 2, 2]               0
      Bottleneck-188           [-1, 1024, 2, 2]               0
          Conv2d-189            [-1, 256, 2, 2]         262,144
     BatchNorm2d-190            [-1, 256, 2, 2]             512
            ReLU-191            [-1, 256, 2, 2]               0
          Linear-192                [-1, 4, 16]           4,096
         Softmax-193                [-1, 4, 16]               0
          Linear-194                [-1, 4, 64]          16,384
     BatchNorm1d-195                [-1, 4, 64]               8
          Linear-196                [-1, 4, 64]          16,384
     BatchNorm2d-197             [-1, 16, 4, 4]              32
     LambdaLayer-198            [-1, 256, 2, 2]               0
     BatchNorm2d-199            [-1, 256, 2, 2]             512
            ReLU-200            [-1, 256, 2, 2]               0
          Conv2d-201           [-1, 1024, 2, 2]         262,144
     BatchNorm2d-202           [-1, 1024, 2, 2]           2,048
            ReLU-203           [-1, 1024, 2, 2]               0
      Bottleneck-204           [-1, 1024, 2, 2]               0
          Conv2d-205            [-1, 256, 2, 2]         262,144
     BatchNorm2d-206            [-1, 256, 2, 2]             512
            ReLU-207            [-1, 256, 2, 2]               0
          Linear-208                [-1, 4, 16]           4,096
         Softmax-209                [-1, 4, 16]               0
          Linear-210                [-1, 4, 64]          16,384
     BatchNorm1d-211                [-1, 4, 64]               8
          Linear-212                [-1, 4, 64]          16,384
     BatchNorm2d-213             [-1, 16, 4, 4]              32
     LambdaLayer-214            [-1, 256, 2, 2]               0
     BatchNorm2d-215            [-1, 256, 2, 2]             512
            ReLU-216            [-1, 256, 2, 2]               0
          Conv2d-217           [-1, 1024, 2, 2]         262,144
     BatchNorm2d-218           [-1, 1024, 2, 2]           2,048
            ReLU-219           [-1, 1024, 2, 2]               0
      Bottleneck-220           [-1, 1024, 2, 2]               0
          Conv2d-221            [-1, 512, 2, 2]         524,288
     BatchNorm2d-222            [-1, 512, 2, 2]           1,024
            ReLU-223            [-1, 512, 2, 2]               0
          Linear-224                [-1, 4, 16]           8,192
         Softmax-225                [-1, 4, 16]               0
          Linear-226               [-1, 4, 128]          65,536
     BatchNorm1d-227               [-1, 4, 128]               8
          Linear-228                [-1, 4, 64]          32,768
     BatchNorm2d-229             [-1, 16, 4, 4]              32
     LambdaLayer-230            [-1, 512, 2, 2]               0
       AvgPool2d-231            [-1, 512, 1, 1]               0
     BatchNorm2d-232            [-1, 512, 1, 1]           1,024
            ReLU-233            [-1, 512, 1, 1]               0
          Conv2d-234           [-1, 2048, 1, 1]       1,048,576
     BatchNorm2d-235           [-1, 2048, 1, 1]           4,096
          Conv2d-236           [-1, 2048, 1, 1]       2,097,152
     BatchNorm2d-237           [-1, 2048, 1, 1]           4,096
            ReLU-238           [-1, 2048, 1, 1]               0
      Bottleneck-239           [-1, 2048, 1, 1]               0
          Conv2d-240            [-1, 512, 1, 1]       1,048,576
     BatchNorm2d-241            [-1, 512, 1, 1]           1,024
            ReLU-242            [-1, 512, 1, 1]               0
          Linear-243                [-1, 1, 16]           8,192
         Softmax-244                [-1, 1, 16]               0
          Linear-245               [-1, 1, 128]          65,536
     BatchNorm1d-246               [-1, 1, 128]               2
          Linear-247                [-1, 1, 64]          32,768
     BatchNorm2d-248             [-1, 16, 1, 4]              32
     LambdaLayer-249            [-1, 512, 1, 1]               0
     BatchNorm2d-250            [-1, 512, 1, 1]           1,024
            ReLU-251            [-1, 512, 1, 1]               0
          Conv2d-252           [-1, 2048, 1, 1]       1,048,576
     BatchNorm2d-253           [-1, 2048, 1, 1]           4,096
            ReLU-254           [-1, 2048, 1, 1]               0
      Bottleneck-255           [-1, 2048, 1, 1]               0
          Conv2d-256            [-1, 512, 1, 1]       1,048,576
     BatchNorm2d-257            [-1, 512, 1, 1]           1,024
            ReLU-258            [-1, 512, 1, 1]               0
          Linear-259                [-1, 1, 16]           8,192
         Softmax-260                [-1, 1, 16]               0
          Linear-261               [-1, 1, 128]          65,536
     BatchNorm1d-262               [-1, 1, 128]               2
          Linear-263                [-1, 1, 64]          32,768
     BatchNorm2d-264             [-1, 16, 1, 4]              32
     LambdaLayer-265            [-1, 512, 1, 1]               0
     BatchNorm2d-266            [-1, 512, 1, 1]           1,024
            ReLU-267            [-1, 512, 1, 1]               0
          Conv2d-268           [-1, 2048, 1, 1]       1,048,576
     BatchNorm2d-269           [-1, 2048, 1, 1]           4,096
            ReLU-270           [-1, 2048, 1, 1]               0
      Bottleneck-271           [-1, 2048, 1, 1]               0
AdaptiveAvgPool2d-272           [-1, 2048, 1, 1]               0
          Linear-273                   [-1, 10]          20,490
================================================================
Total params: 12,828,926
Trainable params: 12,828,926
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 6.54
Params size (MB): 48.94
Estimated Total Size (MB): 55.49
----------------------------------------------------------------
2021-04-03 22:24:43.686895: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
You have CUDA device.
  0% 0/90 [00:00<?, ?it/s]train_acc =  13.752 test_acc =  16.73
train_loss =  2.91 test_loss =  2.45
[0.01]
  1% 1/90 [00:42<1:02:53, 42.40s/it]train_acc =  19.814 test_acc =  24.6
train_loss =  2.49 test_loss =  2.27
[0.015]
  2% 2/90 [01:24<1:02:13, 42.43s/it]train_acc =  23.936 test_acc =  26.42
train_loss =  2.11 test_loss =  1.93
[0.02]
  3% 3/90 [02:07<1:01:27, 42.38s/it]train_acc =  28.872 test_acc =  32.35
train_loss =  1.98 test_loss =  1.82
[0.025]
  4% 4/90 [02:49<1:00:43, 42.36s/it]train_acc =  33.046 test_acc =  35.8
train_loss =  1.9 test_loss =  1.73
[0.02499146325178779]
  6% 5/90 [03:31<59:57, 42.33s/it]  train_acc =  36.066 test_acc =  37.57
train_loss =  1.85 test_loss =  1.69
[0.024965864667322354]
  7% 6/90 [04:14<59:26, 42.45s/it]train_acc =  38.37 test_acc =  40.4
train_loss =  1.81 test_loss =  1.65
[0.024923239211190926]
  8% 7/90 [04:56<58:43, 42.45s/it]train_acc =  40.064 test_acc =  41.26
train_loss =  1.77 test_loss =  1.61
[0.02486364510463933]
  9% 8/90 [05:39<57:54, 42.37s/it]train_acc =  41.702 test_acc =  43.09
train_loss =  1.75 test_loss =  1.57
[0.02478716374604878]
 10% 9/90 [06:21<57:06, 42.30s/it]train_acc =  43.088 test_acc =  44.86
train_loss =  1.72 test_loss =  1.53
[0.024693899599755462]
 11% 10/90 [07:03<56:16, 42.21s/it]train_acc =  44.52 test_acc =  46.19
train_loss =  1.69 test_loss =  1.49
[0.02458398005336485]
 12% 11/90 [07:45<55:44, 42.34s/it]train_acc =  46.08 test_acc =  47.54
train_loss =  1.67 test_loss =  1.46
[0.024457555243755515]
 13% 12/90 [08:28<55:02, 42.34s/it]train_acc =  47.01 test_acc =  49.3
train_loss =  1.65 test_loss =  1.44
[0.02431479785201025]
 14% 13/90 [09:10<54:15, 42.27s/it]train_acc =  48.536 test_acc =  50.16
train_loss =  1.62 test_loss =  1.41
[0.024155902867554452]
 16% 14/90 [09:52<53:34, 42.30s/it]train_acc =  49.648 test_acc =  50.23
train_loss =  1.6 test_loss =  1.4
[0.023981087321824045]
 17% 15/90 [10:35<52:52, 42.30s/it]train_acc =  50.658 test_acc =  51.5
train_loss =  1.58 test_loss =  1.38
[0.023790589991826618]
 18% 16/90 [11:17<52:17, 42.39s/it]train_acc =  51.686 test_acc =  51.43
train_loss =  1.55 test_loss =  1.36
[0.023584671074000783]
 19% 17/90 [11:59<51:34, 42.39s/it]train_acc =  52.936 test_acc =  54.85
train_loss =  1.53 test_loss =  1.3
[0.023363611828819113]
 20% 18/90 [12:42<50:46, 42.31s/it]train_acc =  53.826 test_acc =  54.86
train_loss =  1.51 test_loss =  1.28
[0.02312771419662019]
 21% 19/90 [13:24<50:07, 42.36s/it]train_acc =  55.002 test_acc =  55.38
train_loss =  1.49 test_loss =  1.29
[0.022877300385194393]
 22% 20/90 [14:06<49:18, 42.27s/it]train_acc =  55.636 test_acc =  56.81
train_loss =  1.47 test_loss =  1.25
[0.022612712429686856]
 23% 21/90 [14:49<48:46, 42.41s/it]train_acc =  56.466 test_acc =  56.69
train_loss =  1.46 test_loss =  1.24
[0.022334311725418554]
 24% 22/90 [15:31<47:59, 42.35s/it]train_acc =  57.24 test_acc =  58.01
train_loss =  1.44 test_loss =  1.24
[0.0220424785342638]
 26% 23/90 [16:13<47:15, 42.31s/it]train_acc =  57.624 test_acc =  57.81
train_loss =  1.44 test_loss =  1.21
[0.021737611465258252]
 27% 24/90 [16:55<46:29, 42.27s/it]train_acc =  58.44 test_acc =  57.39
train_loss =  1.42 test_loss =  1.24
[0.02142012693014692]
 28% 25/90 [17:38<45:43, 42.21s/it]train_acc =  59.246 test_acc =  59.2
train_loss =  1.4 test_loss =  1.17
[0.0210904585746158]
 29% 26/90 [18:20<45:11, 42.37s/it]train_acc =  59.646 test_acc =  59.8
train_loss =  1.39 test_loss =  1.16
[0.020749056685984053]
 30% 27/90 [19:03<44:31, 42.41s/it]train_acc =  60.486 test_acc =  59.74
train_loss =  1.38 test_loss =  1.17
[0.020396387578165656]
 31% 28/90 [19:45<43:50, 42.43s/it]train_acc =  61.054 test_acc =  60.49
train_loss =  1.37 test_loss =  1.14
[0.020032932954740724]
 32% 29/90 [20:28<43:10, 42.47s/it]train_acc =  61.734 test_acc =  62.5
train_loss =  1.35 test_loss =  1.11
[0.01965918925100633]
 33% 30/90 [21:10<42:25, 42.43s/it]train_acc =  61.998 test_acc =  63.11
train_loss =  1.34 test_loss =  1.1
[0.019275666955905633]
 34% 31/90 [21:53<41:50, 42.55s/it]train_acc =  62.7 test_acc =  61.73
train_loss =  1.33 test_loss =  1.12
[0.01888288991476135]
 36% 32/90 [22:35<41:06, 42.52s/it]train_acc =  63.084 test_acc =  63.69
train_loss =  1.32 test_loss =  1.07
[0.01848139461376609]
 37% 33/90 [23:18<40:22, 42.50s/it]train_acc =  63.5 test_acc =  63.52
train_loss =  1.32 test_loss =  1.08
[0.018071729447206742]
 38% 34/90 [24:01<39:41, 42.53s/it]train_acc =  64.166 test_acc =  63.82
train_loss =  1.3 test_loss =  1.08
[0.017654453968423807]
 39% 35/90 [24:43<38:53, 42.43s/it]train_acc =  64.534 test_acc =  64.6
train_loss =  1.29 test_loss =  1.05
[0.017230138125528892]
 40% 36/90 [25:26<38:17, 42.55s/it]train_acc =  65.114 test_acc =  63.4
train_loss =  1.28 test_loss =  1.08
[0.016799361482924116]
 41% 37/90 [26:08<37:28, 42.43s/it]train_acc =  65.804 test_acc =  65.39
train_loss =  1.27 test_loss =  1.04
[0.016362712429686857]
 42% 38/90 [26:50<36:41, 42.33s/it]train_acc =  65.646 test_acc =  65.53
train_loss =  1.27 test_loss =  1.01
[0.015920787375901052]
 43% 39/90 [27:32<35:56, 42.29s/it]train_acc =  66.33 test_acc =  65.76
train_loss =  1.26 test_loss =  1.03
[0.015474189938032748]
 44% 40/90 [28:14<35:11, 42.24s/it]train_acc =  66.636 test_acc =  66.14
train_loss =  1.25 test_loss =  1.02
[0.01502353011446264]
 46% 41/90 [28:57<34:37, 42.40s/it]train_acc =  67.216 test_acc =  66.34
train_loss =  1.24 test_loss =  1.01
[0.014569423452301637]
 47% 42/90 [29:39<33:51, 42.32s/it]train_acc =  67.558 test_acc =  66.52
train_loss =  1.23 test_loss =  1.01
[0.014112490206627555]
 48% 43/90 [30:21<33:08, 42.32s/it]train_acc =  67.906 test_acc =  66.41
train_loss =  1.22 test_loss =  1.02
[0.013653354493291288]
 49% 44/90 [31:04<32:27, 42.34s/it]train_acc =  68.138 test_acc =  66.36
train_loss =  1.21 test_loss =  1.0
[0.013192643436449636]
 50% 45/90 [31:46<31:42, 42.27s/it]train_acc =  68.206 test_acc =  67.52
train_loss =  1.21 test_loss =  0.98
[0.012730986311989134]
 51% 46/90 [32:28<31:04, 42.38s/it]train_acc =  68.926 test_acc =  68.83
train_loss =  1.2 test_loss =  0.95
[0.012269013688010888]
 52% 47/90 [33:11<30:21, 42.36s/it]train_acc =  69.036 test_acc =  67.52
train_loss =  1.19 test_loss =  0.98
[0.011807356563550388]
 53% 48/90 [33:53<29:37, 42.31s/it]train_acc =  69.366 test_acc =  67.17
train_loss =  1.19 test_loss =  0.98
[0.011346645506708736]
 54% 49/90 [34:35<28:52, 42.26s/it]train_acc =  69.602 test_acc =  67.47
train_loss =  1.18 test_loss =  0.97
[0.010887509793372472]
 56% 50/90 [35:17<28:10, 42.27s/it]train_acc =  70.23 test_acc =  69.17
train_loss =  1.17 test_loss =  0.95
[0.010430576547698384]
 57% 51/90 [36:00<27:30, 42.33s/it]train_acc =  70.508 test_acc =  68.52
train_loss =  1.16 test_loss =  0.97
[0.009976469885537381]
 58% 52/90 [36:42<26:47, 42.29s/it]train_acc =  70.774 test_acc =  69.09
train_loss =  1.16 test_loss =  0.93
[0.009525810061967278]
 59% 53/90 [37:24<26:03, 42.25s/it]train_acc =  70.99 test_acc =  69.59
train_loss =  1.16 test_loss =  0.93
[0.009079212624098974]
 60% 54/90 [38:07<25:21, 42.28s/it]train_acc =  71.468 test_acc =  69.98
train_loss =  1.15 test_loss =  0.93
[0.008637287570313167]
 61% 55/90 [38:49<24:39, 42.28s/it]train_acc =  71.642 test_acc =  69.48
train_loss =  1.14 test_loss =  0.93
[0.008200638517075906]
 62% 56/90 [39:32<24:02, 42.42s/it]train_acc =  71.928 test_acc =  70.69
train_loss =  1.13 test_loss =  0.9
[0.0077698618744711295]
 63% 57/90 [40:14<23:19, 42.41s/it]train_acc =  72.102 test_acc =  70.59
train_loss =  1.13 test_loss =  0.91
[0.007345546031576215]
 64% 58/90 [40:56<22:36, 42.38s/it]train_acc =  72.362 test_acc =  70.11
train_loss =  1.12 test_loss =  0.91
[0.006928270552793279]
 66% 59/90 [41:38<21:51, 42.31s/it]train_acc =  72.65 test_acc =  70.42
train_loss =  1.12 test_loss =  0.9
[0.006518605386233925]
 67% 60/90 [42:21<21:08, 42.29s/it]train_acc =  72.932 test_acc =  69.57
train_loss =  1.11 test_loss =  0.91
[0.006117110085238671]
 68% 61/90 [43:03<20:30, 42.42s/it]train_acc =  73.162 test_acc =  70.44
train_loss =  1.11 test_loss =  0.9
[0.005724333044094388]
 69% 62/90 [43:46<19:47, 42.42s/it]train_acc =  73.538 test_acc =  70.19
train_loss =  1.1 test_loss =  0.9
[0.0053408107489936905]
 70% 63/90 [44:28<19:06, 42.46s/it]train_acc =  74.016 test_acc =  70.5
train_loss =  1.09 test_loss =  0.9
[0.004967067045259301]
 71% 64/90 [45:11<18:24, 42.47s/it]train_acc =  73.976 test_acc =  70.98
train_loss =  1.09 test_loss =  0.88
[0.00460361242183437]
 72% 65/90 [45:53<17:41, 42.47s/it]train_acc =  74.458 test_acc =  71.27
train_loss =  1.08 test_loss =  0.87
[0.004250943314015974]
 73% 66/90 [46:36<17:00, 42.51s/it]train_acc =  75.16 test_acc =  71.05
train_loss =  1.07 test_loss =  0.89
[0.00390954142538422]
 74% 67/90 [47:18<16:16, 42.47s/it]train_acc =  74.896 test_acc =  71.69
train_loss =  1.07 test_loss =  0.87
[0.0035798730698531]
 76% 68/90 [48:01<15:33, 42.41s/it]train_acc =  75.136 test_acc =  71.79
train_loss =  1.06 test_loss =  0.87
[0.003262388534741766]
 77% 69/90 [48:43<14:50, 42.40s/it]train_acc =  75.506 test_acc =  71.53
train_loss =  1.05 test_loss =  0.87
[0.0029575214657362155]
 78% 70/90 [49:26<14:09, 42.50s/it]train_acc =  75.768 test_acc =  71.91
train_loss =  1.05 test_loss =  0.86
[0.0026656882745814624]
 79% 71/90 [50:08<13:28, 42.58s/it]train_acc =  76.43 test_acc =  71.96
train_loss =  1.04 test_loss =  0.86
[0.002387287570313161]
 80% 72/90 [50:51<12:44, 42.50s/it]train_acc =  76.294 test_acc =  72.5
train_loss =  1.04 test_loss =  0.86
[0.0021226996148056214]
 81% 73/90 [51:33<12:01, 42.44s/it]train_acc =  76.782 test_acc =  71.99
train_loss =  1.03 test_loss =  0.85
[0.0018722858033798273]
 82% 74/90 [52:15<11:17, 42.37s/it]train_acc =  77.128 test_acc =  72.12
train_loss =  1.02 test_loss =  0.86
[0.0016363881711808977]
 83% 75/90 [52:58<10:35, 42.34s/it]train_acc =  77.476 test_acc =  72.6
train_loss =  1.02 test_loss =  0.85
[0.0014153289259992282]
 84% 76/90 [53:40<09:53, 42.41s/it]train_acc =  77.554 test_acc =  72.41
train_loss =  1.01 test_loss =  0.84
[0.001209410008173391]
 86% 77/90 [54:23<09:12, 42.50s/it]train_acc =  77.972 test_acc =  72.41
train_loss =  1.0 test_loss =  0.85
[0.0010189126781759636]
 87% 78/90 [55:05<08:30, 42.53s/it]train_acc =  78.382 test_acc =  72.48
train_loss =  1.0 test_loss =  0.84
[0.0008440971324455535]
 88% 79/90 [55:48<07:48, 42.55s/it]train_acc =  78.304 test_acc =  72.79
train_loss =  1.0 test_loss =  0.84
[0.0006852021479897576]
 89% 80/90 [56:30<07:04, 42.49s/it]train_acc =  78.932 test_acc =  72.72
train_loss =  0.98 test_loss =  0.83
[0.0005424447562444917]
 90% 81/90 [57:13<06:22, 42.52s/it]train_acc =  78.714 test_acc =  72.84
train_loss =  0.99 test_loss =  0.83
[0.0004160199466351578]
 91% 82/90 [57:55<05:39, 42.48s/it]train_acc =  79.386 test_acc =  72.83
train_loss =  0.98 test_loss =  0.84
[0.0003061004002445429]
 92% 83/90 [58:38<04:56, 42.41s/it]train_acc =  79.074 test_acc =  73.12
train_loss =  0.98 test_loss =  0.83
[0.00021283625395122796]
 93% 84/90 [59:20<04:14, 42.36s/it]train_acc =  79.44 test_acc =  73.06
train_loss =  0.97 test_loss =  0.83
[0.0001363548953606757]
 94% 85/90 [1:00:02<03:32, 42.40s/it]train_acc =  79.494 test_acc =  73.0
train_loss =  0.97 test_loss =  0.83
[7.676078880907999e-05]
 96% 86/90 [1:00:46<02:50, 42.63s/it]train_acc =  79.656 test_acc =  73.49
train_loss =  0.96 test_loss =  0.82
[3.413533267764978e-05]
 97% 87/90 [1:01:28<02:07, 42.57s/it]train_acc =  79.872 test_acc =  73.15
train_loss =  0.96 test_loss =  0.82
[8.536748212213687e-06]
 98% 88/90 [1:02:10<01:24, 42.50s/it]train_acc =  79.712 test_acc =  73.15
train_loss =  0.97 test_loss =  0.82
[0.0]
 99% 89/90 [1:02:53<00:42, 42.47s/it]train_acc =  79.854 test_acc =  72.68
train_loss =  0.96 test_loss =  0.83
[1.7073496424427348e-06]
100% 90/90 [1:03:35<00:00, 42.40s/it]
Finished Training